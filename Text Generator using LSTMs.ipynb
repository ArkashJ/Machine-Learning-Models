{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35e9a4fd-4d78-41cf-9f2b-2cb31d0983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f96096e-2117-4101-8315-c126e333cf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/arkashjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a23901c-d115-4809-8123-56a35debbfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 66 files..\n",
      "corpus length: 1915949\n"
     ]
    }
   ],
   "source": [
    "corpora_dir = \"/Users/arkashjain/nltk_data/corpora/state_union\"\n",
    "corpora_dir\n",
    "\n",
    "file_list = []\n",
    "for root, _ , files in os.walk(corpora_dir):\n",
    "    for filename in files:\n",
    "        file_list.append(os.path.join(root, filename))\n",
    "\n",
    "print(\"Read\", len(file_list), \"files..\")\n",
    "\n",
    "docs = []\n",
    "\n",
    "for files in file_list:\n",
    "    with open(files, 'r') as fin:\n",
    "        try:\n",
    "            str_form = fin.read().lower().replace('\\n','')\n",
    "            docs.append(str_form)\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "text = ' '.join(docs)\n",
    "\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0cdea5-90e1-438f-a3d6-da249f7ab153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique characters: 57\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('Total number of unique characters:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars)) #characters to indices\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars)) #indices to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df897fcd-77d4-4192-ae26-eda5ecc8c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences 638637\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "#cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    \n",
    "print('nb sequences', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype = bool)\n",
    "y = np.zeros((len(sentences),len(chars)), dtype = bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "def sample(preds, temperature = 1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print()\n",
    "    print('------ Generating text after Epoch: %d' %epoch)\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- Diversity', diversity)\n",
    "        \n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: ' + sentence + '\"\"')\n",
    "        sys.stdout.write(generated)\n",
    "        \n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1\n",
    "            \n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            \n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            \n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "            \n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "    model.save_weights('saved_weights.hdf5', overwrite=True)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b7cace0-59d2-4439-b115-1de841e5c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Epoch 1/5\n",
      "4989/4990 [============================>.] - ETA: 0s - loss: 2.0940\n",
      "------ Generating text after Epoch: 0\n",
      "----- Diversity 0.2\n",
      "----- Generating with seed: rican americans. i ask congress to refor\"\"\n",
      "rican americans. i ask congress to refore of the resenter and the reges of the resenged to and the reseng to and the readers and and and the reveres and the readers of and the reader and the readers of and the congress of the resendent to and the reader and the resenged and in the congress of and the reader and the reader of and the resend the reader and the rest and the readers and the reader and and the congress of the resporsest and \n",
      "----- Diversity 0.5\n",
      "----- Generating with seed: rican americans. i ask congress to refor\"\"\n",
      "rican americans. i ask congress to refore in the regited in the rase the defing thas year the hargh the mane of and in and american to the cars. and the reale and ervery to lake grents, and the sighes not the governing the world for of remurity and but the readent progection for axeray heard to lanked congress in the readers and the has a dease for the reader and the rest refuned con the reares endressions of and the congression but we \n",
      "----- Diversity 1.0\n",
      "----- Generating with seed: rican americans. i ask congress to refor\"\"\n",
      "rican americans. i ask congress to refora of of memerato.. 'urp, a thunge it that of and archeames can the invere hpansidm of wherely febued to p1k.. the kenthrice. shar seaus of man corerty, to leve venent- op of erispiale's.tare toytrecoon, in, our all delament uscona, intal and in we canug then to the werrs i poose of canmind wiras anganess for a, ous comperic almssice, the is amfitionssew, mmendtity acoingrisgrace and huch peose pea\n",
      "----- Diversity 1.2\n",
      "----- Generating with seed: rican americans. i ask congress to refor\"\"\n",
      "rican americans. i ask congress to refore thhice goobturine stats we poontvenc. ifto ad renegs dranghlefy, who  is to preburitioncan on keape.and rebplenipdald re.viges, arka, hope ago hovery, ad have trako., frinys. of this every, thay ofnerce zage lowld arl kinginust fursome, -fond, te, goopmont andrecato.ingy be, crnedgaborpey.ceafmendmont bications rede bregechey in wistrod coustury os alliof ald- itlataiy lives the mang-whrourently\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "4990/4990 [==============================] - 488s 97ms/step - loss: 2.0940\n",
      "Epoch 2/5\n",
      "4990/4990 [==============================] - ETA: 0s - loss: 1.6357\n",
      "------ Generating text after Epoch: 1\n",
      "----- Diversity 0.2\n",
      "----- Generating with seed: hing america reads, sending literally th\"\"\n",
      "hing america reads, sending literally that we can for the congress of the comming the realons that and the congress of the comming the prosests to the congress and the comminists of the congress and the congress of the congress of our community of the congress of the congress and the comming the security and the congress of the congress of our comming the comming the comminsion of the comming the comminists and the congress and the comm\n",
      "----- Diversity 0.5\n",
      "----- Generating with seed: hing america reads, sending literally th\"\"\n",
      "hing america reads, sending literally that and many defensent as new country of have and comitions of the congress is the world and commistives and can fell american and the programs of the for the americans new to the compension with a continue sound but in the congress we can te production in the congress and whone of a procram courtration of the commistion of the programs in the fiscal are resporsing to the world continues to the con\n",
      "----- Diversity 1.0\n",
      "----- Generating with seed: hing america reads, sending literally th\"\"\n",
      "hing america reads, sending literally the podate easter of owe wime mosh baiging some was affect stritle-nearsy on the continies.inoliful has all andaling leaders of ecentiny to elure that shibling out laysing mosts and taxace wolkn oulled not mitthonive willops antion, our addinipstons of befice e interted to dos ffor out as antejure. we seve of yauch sirethis to provide is atstate of batition lepares produces that his iscalsh proweds \n",
      "----- Diversity 1.2\n",
      "----- Generating with seed: hing america reads, sending literally th\"\"\n",
      "hing america reads, sending literally thosist sabiticy if the amerisaningm i sedes in ruwandy nation cannities in the fatial solfors adsuldsnce anvilueds of opromicity sore of muginowiof droud, e tony furm inchoudatecaly on luggem. we natriur.  plas dodah, that telp.ap notion to kely to dracht ay injose of havings for oul penacopy ac5 fiming eefongs, ad thes cemare sa aily hirh petinute, ourshope agurnansal to goway, has fey ligdicy pla\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "4990/4990 [==============================] - 506s 101ms/step - loss: 1.6357\n",
      "Epoch 3/5\n",
      "4989/4990 [============================>.] - ETA: 0s - loss: 1.4737\n",
      "------ Generating text after Epoch: 2\n",
      "----- Diversity 0.2\n",
      "----- Generating with seed: for revision of the immigration laws wil\"\"\n",
      "for revision of the immigration laws will be recond of the program to the production in the congress of the fight to the families in the destrice the security of the prices of the united states of the that the world of the states of the state of the families and the state of the fight to the same of the the that the peace of the world and the state of the state of the congress and the states of the that we make the state of the american\n",
      "----- Diversity 0.5\n",
      "----- Generating with seed: for revision of the immigration laws wil\"\"\n",
      "for revision of the immigration laws will be a great can federal production in this people be middly will end the past you have do not to achieve the world and our strength of the development that the indevertanies in this cannot the consention and basines to the ension and propose them to invertand mank on the defense of the increase of the more sems see in the senvice, or program and the american people is a part before and last years\n",
      "----- Diversity 1.0\n",
      "----- Generating with seed: for revision of the immigration laws wil\"\"\n",
      "for revision of the immigration laws will inclitions and become oth(applicy clace to membair the reth yinally becausing, welk commuting call or any pregident. by be more sere you as the -ot stanged wall-propose an amold. this the stand of these leads in rese of enecolian for our effort. and it house lost in increase which and the reviurts of ockica leadul nor religination of the belise, a ofmerd and to argonimitary soll, that the world.\n",
      "----- Diversity 1.2\n",
      "----- Generating with seed: for revision of the immigration laws wil\"\"\n",
      "for revision of the immigration laws will nation.i well.. stimilly apriess murding.in toconce chance all those wow--we than no thingb8ural dene. v thank vawich- our the bigitged by recendures.are rejaid in ancess of of throinge of that thre0 belys may the the tho seep they'reswing knact office . rassureging helf that we shopth both tory efferen orcentays and ryarce. no notcem, ourading our omenwition, and i mugnoridf wereds; we orver. i\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "4990/4990 [==============================] - 485s 97ms/step - loss: 1.4737\n",
      "Epoch 4/5\n",
      "4990/4990 [==============================] - ETA: 0s - loss: 1.3876\n",
      "------ Generating text after Epoch: 3\n",
      "----- Diversity 0.2\n",
      "----- Generating with seed:  being matched by the emergence of new t\"\"\n",
      " being matched by the emergence of new the success and the fair and success of the congress to the congress to the first and the country and a great reserve the congress to have a community in the community and the congress of the congress and the congress and the congress and the country and the congress to the congress to the congress of the congress to the first and the congress to the congress to the congress to a sure the congress \n",
      "----- Diversity 0.5\n",
      "----- Generating with seed:  being matched by the emergence of new t\"\"\n",
      " being matched by the emergence of new the many for the security, dove measures and the price in the many of our resources of a way and economy of the befene and working have seen to great the than a right action of the congress to the congress to increased a discans of the realin of the states and our successis and land, i am in the friences are a country of the fiscal into the congress to the union in all of the support and develop an\n",
      "----- Diversity 1.0\n",
      "----- Generating with seed:  being matched by the emergence of new t\"\"\n",
      " being matched by the emergence of new this year. to musare controut democraticy, the baggerswapip is all weyst're greatures of duriches. we're our nuil on a made good, sciplan0ly america. for toggatoly will woult do new alon, it is these has steprer about the commonn, i must and toda, and reathing a session of incompaidencsuprouls continually proserts.we seek to go other healthed by foreally falt of itwirs never high hource to subsion \n",
      "----- Diversity 1.2\n",
      "----- Generating with seed:  being matched by the emergence of new t\"\"\n",
      " being matched by the emergence of new the initiational is trainity develoument to tealprovy dene will protol. i naty mil have rud by to yof being ty heoping, every celnciliagation. dur'nds oucled, are fourty hatiral inflation jos- i having come hose reparrots and staping facing on the succhmacian, we must ot nurgor for meacteref givatpors our balable agienal cormunitgefoved zight to fut. youngy, alew andnrwe-mann our hoarthed policies.\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "4990/4990 [==============================] - 499s 100ms/step - loss: 1.3876\n",
      "Epoch 5/5\n",
      "4989/4990 [============================>.] - ETA: 0s - loss: 1.3360\n",
      "------ Generating text after Epoch: 4\n",
      "----- Diversity 0.2\n",
      "----- Generating with seed: very human activity is pressed into serv\"\"\n",
      "very human activity is pressed into serve our country and the programs to the state of the congress and the state of the united states and the same of the first of the committen resources and the state of the state of the world and to the congress the state of the world. the prosperity of the many of the military for a serious of our country and the american people and the congress and the common state of the congress to the american pe\n",
      "----- Diversity 0.5\n",
      "----- Generating with seed: very human activity is pressed into serv\"\"\n",
      "very human activity is pressed into service the congress in the budget for an established to the congress on the senaration of attack of the basic proposes and the any of children and the world and congress on its communities and come to help the same on the folution of the congress, and the first war deficit the high product the programs in the many of the substaning the fact both the social security. the tax free world to the internat\n",
      "----- Diversity 1.0\n",
      "----- Generating with seed: very human activity is pressed into serv\"\"\n",
      "very human activity is pressed into serve better ard people are somethip has closed this farlend task now feer grainet of our will he ling rome any onge fasce doy's home resists age.no magement of a prigations. it will be the federal day and male. laces the obla --we are what clear undelstmonted and terrorists, and sa certain the which will be century , by insucturated sently economic contectic limes, that we we hape, i should. the righ\n",
      "----- Diversity 1.2\n",
      "----- Generating with seed: very human activity is pressed into serv\"\"\n",
      "very human activity is pressed into serve on mrihupe stopt many evirernay will befole valunesship toving cut this fights, least victory quds. as citibers inqumatle; bo povinithe and out, but qued yetery country lastouh, or veetwrentw, non impasir hidfier creatista cuses and residve a peafomon free efficiens. in a new mucomand out ally to youth, our citual enduble and if many athans whonxer0 federal seconding this yefices, tocorn and imm\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "4990/4990 [==============================] - 464s 93ms/step - loss: 1.3360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae998c70d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Building model...')\n",
    "#size of the vector in the hidden layer\n",
    "hidden_size = 128\n",
    "# Initializing model sequence\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_size, input_shape=(maxlen, len(chars))))\n",
    "\n",
    "#Adding the output layer that is softmax of the number of characters\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "#optimization through RMSprop\n",
    "optimizer_new = RMSprop()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer_new)\n",
    "\n",
    "model.fit(x, y, \n",
    "          batch_size = 128, \n",
    "          epochs = 5, \n",
    "          callbacks = [print_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73166a-287d-4354-ac12-5ae3c10b77f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1117/4990 [=====>........................] - ETA: 4:45 - loss: 1.3004"
     ]
    }
   ],
   "source": [
    "#for continued training\n",
    "model.load_weights(\"saved_weights.hdf5\")\n",
    "\n",
    "model.fit(x, y, \n",
    "          batch_size = 128, \n",
    "          epochs = 5, \n",
    "          callbacks = [print_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a727ad-1fff-4ef2-b127-104e85e375bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751aaf48-4d86-43c8-9158-e575234077a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f30ea8-2d69-42d2-83a9-ca601b68087d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c43c6-a999-4115-a64b-9da3ac21f3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea5f0b-b514-4f48-a443-e0d3d3f13502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b40fa9-7141-4ee3-b4b1-68742a11bd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b8973-bc9b-490d-bb9e-fb6f519b3bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ad804-a1ee-46b9-a76d-dc93df0d634b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b41ee-2bf5-4ad4-8bdc-d085ec720c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf6d83-65a9-4abf-9ed9-3c3bb744f86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
